{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-ai-learning-journey","title":"\ud83d\udc4b Welcome to the AI Learning Journey","text":"<p>This site captures my 5-day sprint into AI \u2014 from foundations to agents \u2014 so others can learn alongside me.</p>"},{"location":"#goals","title":"\ud83c\udf1f Goals","text":"<ul> <li>Build a solid foundation (tokenization, embeddings, transformers)</li> <li>Practice prompting techniques with hands-on examples</li> <li>Explore LLMs (open models &amp; hosted APIs)</li> <li>Understand agents &amp; multi-agents in workflows</li> <li>Share mini-projects as showcases</li> </ul>"},{"location":"#5-day-plan","title":"\ud83d\udcc5 5-Day Plan","text":"Day Focus Key Topics Day 1 Basics foundations, vectors, embeddings, tokenization Day 2 Prompting techniques, examples, anti-patterns Day 3 Transformers &amp; LLMs attention, fine-tuning, model choices Day 4 Agents tools, memory, workflows Day 5 Multi-Agents + Showcase coordination patterns, case studies, projects"},{"location":"#what-youll-find-here","title":"\ud83e\udded What you\u2019ll find here","text":"<ul> <li>Basics \u2192 foundations like tokenization, embeddings, and vectors  </li> <li>Prompting \u2192 techniques, examples, and pitfalls  </li> <li>Transformers \u2192 attention, training, fine-tuning  </li> <li>LLMs \u2192 open vs hosted models  </li> <li>Agents \u2192 tools, memory, workflows  </li> <li>Multi-Agents \u2192 collaboration patterns  </li> <li>Showcase \u2192 small projects &amp; demos built during this journey  </li> </ul> <p>How to use this site</p> <p>Browse day-by-day, or jump into a topic you\u2019re curious about. Each page has concept notes, code snippets, exercises, and takeaways.</p>"},{"location":"agents/","title":"Agents","text":"<p>Agents extend Large Language Models (LLMs) by giving them the ability to use tools, plan tasks, and remember context. They are central to building AI systems that interact with the real world.</p>"},{"location":"agents/#core-building-blocks","title":"Core Building Blocks","text":"<ul> <li>LLM (Brain) \u2192 interprets instructions and reasons about tasks  </li> <li>Tools (Hands) \u2192 external APIs, calculators, databases, search engines  </li> <li>Memory \u2192 short-term or long-term storage of information  </li> <li>Planner \u2192 decides the sequence of steps  </li> <li>Executor \u2192 carries out the actions and returns results  </li> </ul>"},{"location":"agents/#example-workflow","title":"Example Workflow","text":"<p>Prompt: \u201cFind the population of France, then calculate 10% of it.\u201d</p> <p>Agent process:</p> <ol> <li>Plan: Identify steps \u2192 (a) search for population, (b) calculate 10%.  </li> <li>Tool use: Search API \u2192 returns \u201c67 million.\u201d  </li> <li>Reasoning: 10% of 67 million = 6.7 million.  </li> <li>Answer: <pre><code>10% of France\u2019s population \u2248 6.7 million\n</code></pre></li> </ol>"},{"location":"agents/#why-use-agents","title":"Why Use Agents?","text":"<p>LLMs alone cannot do accurate math, access the web, or update knowledge. Agents combine reasoning with external actions, making them more useful in practice. Applications: assistants, research copilots, robotic process automation (RPA).</p>"},{"location":"agents/#types-of-agents","title":"Types of Agents","text":"<ul> <li>Single-agent \u2192 one model with tools</li> <li>Multi-agent systems \u2192 multiple agents collaborating</li> <li>Specialized agents \u2192 focused on specific tasks (e.g., SQL agent, travel agent)</li> </ul>"},{"location":"agents/#challenges","title":"Challenges","text":"<ul> <li>Error propagation \u2192 mistakes in one step can affect the final result</li> <li>Latency \u2192 multiple tool calls can slow down responses</li> <li>Security \u2192 tool use may involve external risks</li> <li>Control \u2192 preventing infinite loops or unintended actions</li> </ul>"},{"location":"agents/#further-reads","title":"Further Reads","text":"<p>ReAct: Reason + Act</p> <p>LangChain Agents Documentation</p> <p>AutoGPT (GitHub)</p>"},{"location":"basics/","title":"Basics of AI","text":"<p>This section introduces the core concepts of Artificial Intelligence (AI) in a simple and accessible way. It provides the foundation needed before exploring prompting, transformers, large language models, and agentic systems. The focus here is on understanding, not on heavy mathematics or coding.</p>"},{"location":"basics/#1-what-is-ai","title":"1. What is AI","text":"<p>Artificial Intelligence (AI) is the broad field of creating systems that appear intelligent. Machine Learning (ML) is a subset of AI where systems learn patterns from data instead of following hard-coded rules. Deep Learning (DL) is a further subset of ML that uses multi-layered neural networks to model complex relationships.</p> <p>Example:   - AI: A chess-playing computer.   - ML: An email spam filter trained on examples.   - DL: An image recognition model that identifies cats and dogs.</p>"},{"location":"basics/#2-types-of-learning","title":"2. Types of Learning","text":"<p>There are three main approaches to how systems learn:</p> <p>Supervised Learning: Training with labeled data, where both inputs and correct outputs are provided. Unsupervised Learning: Discovering hidden patterns in unlabeled data. Reinforcement Learning: Learning through trial and error by receiving rewards or penalties for actions.</p>"},{"location":"basics/#3-data-fundamentals","title":"3. Data Fundamentals","text":"<p>Data is the foundation of AI. Good data leads to good models.  </p> <p>Structured Data: Organized information in rows and columns, such as spreadsheets. Unstructured Data: Free-form information such as text, images, and audio. Preprocessing: Cleaning, normalizing, and tokenizing raw inputs before use. Splitting: Dividing data into training, validation, and test sets to ensure fair evaluation.</p>"},{"location":"basics/#4-mathematical-intuition","title":"4. Mathematical Intuition","text":"<p>A little mathematical intuition helps in understanding AI concepts.  </p> <p>Vectors and Matrices: Representations of data as arrows and grids of numbers. Probability: A way to handle uncertainty and measure likelihood. Gradient Descent: The process of gradually adjusting parameters to minimize error, like rolling downhill to the lowest point in a valley.</p>"},{"location":"basics/#5-neural-network-basics","title":"5. Neural Network Basics","text":"<p>Neural networks are the building blocks of modern deep learning.  </p> <p>Neuron: A unit that takes input, applies weights, and produces output through an activation function. Layer: A collection of neurons. Multiple layers form a network. Forward Pass: The flow of input through the network to produce output. Backpropagation: The adjustment of weights by propagating errors backward to improve predictions.</p>"},{"location":"basics/#6-tools-and-ecosystem","title":"6. Tools and Ecosystem","text":"<p>AI development has a strong ecosystem of tools and frameworks.  </p> <p>Python is the primary programming language for AI. Jupyter Notebooks are widely used for interactive exploration and experiments. Common libraries include NumPy for numerical operations, pandas for data handling, scikit-learn for traditional ML, PyTorch and TensorFlow for deep learning, and Hugging Face for working with modern language models.</p>"},{"location":"basics/#7-responsible-ai","title":"7. Responsible AI","text":"<p>Building AI responsibly is as important as building it effectively.  </p> <p>Models may learn bias from data. Privacy must be protected when working with sensitive information. Transparency and explainability are needed so results are not \u201cblack boxes.\u201d Safety measures ensure systems do not produce harmful or misleading outcomes.</p>"},{"location":"basics/#8-summary","title":"8. Summary","text":"<p>The basics of AI cover the following ideas:  </p> <ul> <li>AI, ML, and DL are related but distinct concepts.  </li> <li>Learning can be supervised, unsupervised, or reinforcement-based.  </li> <li>Data quality and preparation are critical.  </li> <li>Some mathematical intuition helps in understanding how models work.  </li> <li>Neural networks are built from simple components that are stacked into deep systems.  </li> <li>Tools and libraries form the backbone of practical AI work.  </li> <li>Ethics and responsibility are essential in real-world applications.</li> </ul> <p>With this foundation in place, the next step is Prompting, which explores how to communicate effectively with AI systems.</p>"},{"location":"llms/","title":"Large Language Models (LLMs)","text":"<p>Large Language Models (LLMs) are neural networks based on the Transformer architecture, trained on massive text datasets to predict the next token in a sequence. They form the backbone of modern AI systems, powering applications from chatbots to code assistants.</p>"},{"location":"llms/#visualization","title":"Visualization","text":"<p>The diagram below summarizes high level architecural flow in Large Language Models (LLMs).</p> <p></p>"},{"location":"llms/#what-it-shows","title":"What it shows","text":"<p>Training (left)</p> <ol> <li>Training data (text corpus) \u2014 books, websites, code, papers.</li> <li>Tokenization \u2014 text \u2192 tokens (e.g., <code>\"Hello world!\" \u2192 [15496, 995]</code>).</li> <li>Transformer layers \u2014 stacked blocks of self-attention + feed-forward networks learn patterns.</li> <li>Trained model \u2014 learned weights are saved for later use.</li> </ol> <p>Inference (right)</p> <ol> <li>Input prompt \u2014 e.g., \u201cWhat is the capital of France?\u201d</li> <li>Tokenization \u2014 prompt \u2192 tokens.</li> <li>Model \u2014 forward pass through the same transformer stack (no training).</li> <li>Output tokens \u2192 text \u2014 tokens decode to an answer (e.g., \u201cParis\u201d).</li> </ol> <p>Key idea:  Training learns statistical patterns from massive data; inference applies those patterns to new prompts.</p>"},{"location":"llms/#types-of-llms","title":"Types of LLMs","text":"<ul> <li>Open models \u2192 Downloadable and runnable locally  <ul> <li>Examples: LLaMA, Mistral, Falcon, Gemma  </li> </ul> </li> <li>Hosted APIs \u2192 Accessible via cloud services  <ul> <li>Examples: GPT (OpenAI), Claude (Anthropic), Gemini (Google)  </li> </ul> </li> <li>Domain-specific models \u2192 Trained for specialized use cases  <ul> <li>Examples: BioGPT (medical), StarCoder (code)  </li> </ul> </li> </ul>"},{"location":"llms/#capabilities","title":"Capabilities","text":"<ul> <li>Text generation: summaries, stories, explanations  </li> <li>Question answering: knowledge-based responses  </li> <li>Classification: spam detection, sentiment analysis  </li> <li>Translation: cross-lingual tasks  </li> <li>Reasoning: math, logic, planning  </li> <li>Tool use: via prompts or agent frameworks  </li> </ul>"},{"location":"llms/#limitations","title":"Limitations","text":"<ul> <li>Hallucination \u2192 May produce factually incorrect outputs  </li> <li>Bias \u2192 Reflects societal and dataset biases  </li> <li>Context window limits \u2192 Only process a fixed number of tokens at once  </li> <li>Knowledge staleness \u2192 Models only know up to their training cut-off  </li> </ul>"},{"location":"llms/#scale-and-parameters","title":"Scale and Parameters","text":"<ul> <li>LLMs are characterized by their parameter count:  <ul> <li>Small (1B\u20137B) \u2192 Lightweight, runs on laptops/GPUs  </li> <li>Medium (13B\u201334B) \u2192 More capable, needs good hardware  </li> <li>Large (65B\u2013540B) \u2192 State-of-the-art, requires distributed clusters  </li> </ul> </li> </ul> <p>Trade-off: larger = more accurate and capable, but slower and more costly.</p>"},{"location":"llms/#deployment-modes","title":"Deployment Modes","text":"<ul> <li>Cloud APIs \u2192 Simple setup, scalable, but data leaves your environment  </li> <li>On-prem / local \u2192 Privacy control, but high compute cost  </li> <li>Hybrid (RAG) \u2192 Retrieval-augmented generation extends smaller models with external knowledge  </li> </ul>"},{"location":"llms/#references","title":"References","text":"<ul> <li>A Survey of Large Language Models (Zhao et al., 2023) </li> <li>Hugging Face Model Hub </li> <li>Stanford HELM Benchmark </li> </ul>"},{"location":"multi-agents/","title":"Multi-Agent Systems","text":"<p>Multi-agent systems (MAS) extend the concept of a single agent by allowing multiple agents to work together, either cooperatively or competitively, to achieve complex goals. These systems mirror real-world scenarios where no single entity has full knowledge or complete capability, but together they can solve problems more effectively.</p>"},{"location":"multi-agents/#1-what-are-multi-agent-systems","title":"1. What Are Multi-Agent Systems?","text":"<p>A multi-agent system is a collection of autonomous agents that:</p> <ul> <li>Perceive their environment</li> <li>Make decisions individually</li> <li>Interact with other agents</li> <li>Coordinate to accomplish shared or individual goals</li> </ul> <p>In AI, these systems are used for tasks such as:</p> <ul> <li>Distributed problem solving</li> <li>Negotiation and planning</li> <li>Large-scale simulations</li> <li>Workflow automation</li> </ul>"},{"location":"multi-agents/#2-key-characteristics","title":"2. Key Characteristics","text":"<ul> <li>Autonomy: Each agent operates independently.  </li> <li>Collaboration: Agents may share knowledge and resources.  </li> <li>Communication: Agents use protocols to exchange information.  </li> <li>Coordination: Agents divide tasks, avoid conflicts, and plan execution.  </li> <li>Scalability: More agents can be added to expand capabilities.  </li> </ul>"},{"location":"multi-agents/#3-architectures-of-multi-agent-systems","title":"3. Architectures of Multi-Agent Systems","text":"<ol> <li> <p>Centralized Coordination: A leader or controller orchestrates actions.  </p> <ul> <li>Simpler but less robust.</li> </ul> </li> <li> <p>Decentralized / Distributed : Agents act independently and negotiate through communication.  </p> <ul> <li>More robust, scalable, and realistic for AI ecosystems.</li> </ul> </li> <li> <p>Hybrid : Combines centralized planning with distributed execution.  </p> </li> </ol>"},{"location":"multi-agents/#4-model-context-protocol-mcp","title":"4. Model Context Protocol (MCP)","text":"<p>MCP provides a standardized way for agents and tools to share context.  </p> <ul> <li>Ensures that each agent can access relevant information consistently.  </li> <li>Avoids duplication of state or knowledge.  </li> <li>Allows agents to plug into workflows without custom integrations.  </li> </ul> <p>Example use case: A Case Planner Agent retrieves knowledge from a database using MCP. The same context is then available to a Validator Agent without re-fetching or duplicating logic.</p> <p>Benefits: </p> <ul> <li>Reusability  </li> <li>Interoperability  </li> <li>Reduced friction when connecting different systems  </li> </ul>"},{"location":"multi-agents/#5-agent-to-agent-a2a-communication","title":"5. Agent-to-Agent (A2A) Communication","text":"<p>A2A defines how agents talk to each other directly. </p> <ul> <li>Similar to APIs, but designed for autonomous coordination.  </li> <li>Supports message passing, negotiation, and delegation of tasks.  </li> </ul> <p>Example flow:  </p> <ol> <li>A Data Retrieval Agent fetches information.  </li> <li>A Reasoning Agent analyzes the data.  </li> <li>A Report Agent generates a final human-readable summary.  </li> </ol> <p>Agents don\u2019t need to be tightly coupled \u2014 they just need to understand the A2A protocol.</p>"},{"location":"multi-agents/#6-mas-mcp-a2a-in-practice","title":"6. MAS + MCP + A2A in Practice","text":"<p>When combined:</p> <ul> <li>MCP standardizes the shared context (common memory/state).  </li> <li>A2A enables direct communication between agents.  </li> <li>MAS orchestrates multiple agents with different roles to solve end-to-end problems.  </li> </ul> <p>This design supports complex workflows like:</p> <ul> <li>Customer service automation with specialized agents (FAQ, escalation, analytics)  </li> <li>AI-powered research assistants collaborating on literature reviews  </li> <li>Credit card chip data validation where agents handle decoding, rule-checking, and reporting  </li> </ul>"},{"location":"multi-agents/#7-use-cases-in-ai","title":"7. Use Cases in AI","text":"<ul> <li>Autonomous Research: Different agents for searching, summarizing, and fact-checking.  </li> <li>Fraud Detection: Agents monitoring transactions from multiple perspectives.  </li> <li>Smart Personal Assistants: A scheduling agent coordinating with a travel agent and finance agent.  </li> <li>Multi-Agent RAG: Planner, retriever, and summarizer agents collaborating in a retrieval-augmented workflow.  </li> </ul>"},{"location":"multi-agents/#8-challenges","title":"8. Challenges","text":"<ul> <li>Communication overhead  </li> <li>Alignment on goals and incentives  </li> <li>Error propagation across agents  </li> <li>Security and trust in decentralized agent environments  </li> </ul>"},{"location":"multi-agents/#9-further-deep-dives-references","title":"9. Further Deep Dives &amp; References","text":"<ul> <li>LangGraph Multi-Agent Patterns </li> <li>Model Context Protocol (MCP) Proposal </li> <li>Agent-to-Agent (A2A) Interactions </li> </ul>"},{"location":"prompting/","title":"Prompting","text":"<p>Prompting is the practice of instructing a large language model (LLM) to perform a task. The structure and clarity of the prompt strongly influence the quality of the response.</p>"},{"location":"prompting/#core-principles","title":"Core Principles","text":"<ol> <li> <p>Role \u2014 define the model\u2019s persona  </p> <p>\u201cYou are a helpful Python tutor.\u201d </p> </li> <li> <p>Goal \u2014 specify the exact task  </p> <p>\u201cExplain decorators in simple terms.\u201d </p> </li> <li> <p>Constraints \u2014 add rules  </p> <p>\u201cUse bullet points, max 5 lines.\u201d </p> </li> <li> <p>Format \u2014 define output structure  </p> <p>\u201cReturn JSON with keys: title, summary.\u201d </p> </li> <li> <p>Examples (few-shot) \u2014 show inputs &amp; outputs  </p> <pre><code>English: Hello \u2192 French: Bonjour  \nEnglish: Thank you \u2192 French: Merci  \nEnglish: Good night \u2192 French:\n</code></pre> </li> </ol>"},{"location":"prompting/#prompting-techniques","title":"Prompting Techniques","text":"<ul> <li> <p>Zero-shot \u2192 Direct instruction without examples Prompt: <pre><code>Translate \"machine learning\" into Spanish.\n</code></pre> Output: <pre><code>aprendizaje autom\u00e1tico\n</code></pre></p> </li> <li> <p>Few-shot \u2192 Include examples to guide behavior Prompt: <pre><code>English: Hello \u2192 French: Bonjour\nEnglish: Thank you \u2192 French: Merci\nEnglish: Good night \u2192 French:\n</code></pre> Output: <pre><code>Bonne nuit\n</code></pre></p> </li> <li>Chain of Thought (CoT) \u2192 Ask for step-by-step reasoning Prompt: <pre><code>Solve step by step:\nA pen costs $1 and a notebook costs $2. \nIf I buy 3 pens and 2 notebooks, how much do I spend in total?\n</code></pre> Output: <pre><code>Step 1: Cost of 3 pens = 3 \u00d7 $1 = $3\nStep 2: Cost of 2 notebooks = 2 \u00d7 $2 = $4\nStep 3: Total = $3 + $4 = $7\nFinal Answer: $7\n</code></pre></li> <li>ReAct \u2192 Reasoning combined with external actions/tools  Prompt: <pre><code>Task: Find out who won the 2018 FIFA World Cup and explain in one sentence.\nPlan: \n1. Search for the answer.\n2. Return a short explanation.\n</code></pre> Output: <pre><code>The 2018 FIFA World Cup was won by France, who defeated Croatia 4\u20132 in the final.\n</code></pre></li> <li>Self-critique \u2192 generate, evaluate, and refine outputs Prompt: <pre><code>Write a two-sentence summary of what transformers are. \nThen critique the summary. \nFinally, provide an improved version.\n</code></pre> Output: <pre><code>Initial Summary:\nTransformers are deep learning models that use attention to process sequences. \nThey are widely used for NLP tasks.\n\nCritique:\nThe explanation is too brief and lacks context about their significance.\n\nImproved Summary:\nTransformers are neural network architectures built around self-attention, \nallowing them to model long-range dependencies in text efficiently. \nThey form the foundation of modern large language models used in NLP and beyond.\n</code></pre></li> </ul>"},{"location":"prompting/#anti-patterns","title":"Anti-Patterns","text":"<ul> <li>Vague requests (\u201cExplain AI.\u201d)  </li> <li>Long prompts with key info buried at the end  </li> <li>Implicit requirements not stated clearly  </li> <li>Missing or inconsistent output format  </li> </ul>"},{"location":"prompting/#why-prompting-matters","title":"Why Prompting Matters","text":"<ul> <li>Saves tokens \u2192 lower cost &amp; faster responses</li> <li>Produces predictable outputs</li> <li>Enables automation and evaluation when outputs are structured</li> </ul>"},{"location":"prompting/#deep-dive","title":"Deep Dive","text":"<ul> <li>Prompt Engineering Guide</li> <li>OpenAI Cookbook</li> <li>Chain of Thought Reasoning (Wei et al., 2022)</li> </ul>"},{"location":"rag/","title":"Retrieval-Augmented Generation (RAG)","text":"<p>Retrieval-Augmented Generation (RAG) is a method to extend Large Language Models (LLMs) with external knowledge sources. Instead of relying only on pre-trained data, RAG retrieves relevant information at runtime and injects it into the model\u2019s context.</p>"},{"location":"rag/#why-rag","title":"Why RAG?","text":"<ul> <li>LLMs may hallucinate when knowledge is missing.  </li> <li>Training or fine-tuning large models is expensive.  </li> <li>RAG provides fresh, domain-specific knowledge on demand.  </li> </ul> <p>Example: \u201cSummarize our company\u2019s Q1 financial report.\u201d </p> <ul> <li>Plain LLM \u2192 may hallucinate.  </li> <li>RAG \u2192 retrieves the actual report \u2192 summary is grounded in real data.  </li> </ul>"},{"location":"rag/#workflow","title":"Workflow","text":"<ol> <li>Chunk documents \u2192 Split into manageable passages (e.g., 300\u2013500 tokens).  </li> <li>Embed chunks \u2192 Convert text into vectors using an embedding model.  </li> <li>Store vectors \u2192 Save in a vector database (FAISS, PGVector, Pinecone, Weaviate).  </li> <li>User query \u2192 Embed the question into the same vector space.  </li> <li>Retrieve \u2192 Find the most similar chunks using cosine similarity.  </li> <li>Augment prompt \u2192 Attach the retrieved chunks as context.  </li> <li>Generate \u2192 LLM uses both the prompt and retrieved context to produce an answer.  </li> </ol>"},{"location":"rag/#example-prompt","title":"Example Prompt","text":"<pre><code>You are a helpful assistant. \nUse only the provided context to answer the question.\n\n# Question:\nWho is the CEO of OpenAI?\n\n# Context:\n1. OpenAI Leadership (2023): Sam Altman is the CEO.\n2. OpenAI Board Notes: The company is based in San Francisco.\n\n# Instructions:\n- If the answer is not in the context, say \u201cI don\u2019t know.\u201d\n- Be concise.\n</code></pre>"},{"location":"rag/#strengths","title":"Strengths","text":"<ul> <li>Reduces hallucination.</li> <li>Provides fresh and updated knowledge.</li> <li>Allows smaller models to behave like larger ones.</li> </ul>"},{"location":"rag/#challenges","title":"Challenges","text":"<ul> <li>Chunking strategy affects retrieval quality.</li> <li>Embedding quality determines similarity search performance.</li> <li>Context window limits restrict how much context can be passed.</li> <li>Latency from vector search can add overhead.</li> </ul>"},{"location":"rag/#further-reads","title":"Further Reads","text":"<p>Retrieval-Augmented Generation (Lewis et al., 2020)</p> <p>Pinecone: What is RAG?</p> <p>Hugging Face: RAG Models</p>"},{"location":"showcase/","title":"Showcase","text":""},{"location":"showcase/#rag-mini-bm25-vs-embeddings","title":"RAG Mini \u2014 BM25 vs Embeddings","text":"<p>Try it live below (embedded from Hugging Face Spaces):</p> <p> </p> <p>If the embedded app doesn\u2019t load, open it directly: https://huggingface.co/spaces/pantji/pankaj_rag-mini</p>"},{"location":"showcase/mcp-a2a-demo/","title":"MCP + A2A Demo","text":"<p>Below is a live Hugging Face Space embedded directly:</p> <p> </p>"},{"location":"tokenization/","title":"Tokenization","text":"<p>Tokenization is the process of breaking down text into smaller units called tokens. Large Language Models (LLMs) do not operate directly on words, but on these tokens. Understanding tokenization is critical for working with modern AI systems because it determines how text is represented, how much a model can process at once, and how costs are calculated.</p>"},{"location":"tokenization/#1-what-is-a-token","title":"1. What is a Token","text":"<p>A token is a chunk of text that a model can process. It may be as short as a single character or as long as a common word.  </p> <p>Examples: - The word \u201ccat\u201d is usually one token. - The word \u201cunhappiness\u201d may become two tokens: \u201cun\u201d + \u201chappiness.\u201d - Emojis and special symbols are also tokens.  </p>"},{"location":"tokenization/#2-why-tokenization-matters","title":"2. Why Tokenization Matters","text":"<ul> <li>Model Input: Models only understand tokens, not raw text.  </li> <li>Context Window: The maximum number of tokens a model can handle in one request.  </li> <li>Costs: Billing for API usage is typically based on tokens, not characters or words.  </li> <li>Performance: Efficient tokenization helps models process information faster and with less memory.  </li> </ul>"},{"location":"tokenization/#3-types-of-tokenization","title":"3. Types of Tokenization","text":"<ol> <li> <p>Word-Level Tokenization    Splits text at spaces and punctuation. Simple but inefficient for rare words.  </p> </li> <li> <p>Subword-Level Tokenization    Breaks words into frequent sub-parts using methods like Byte Pair Encoding (BPE), WordPiece, or SentencePiece.  </p> <ul> <li>Example: \u201cunhappiness\u201d \u2192 \u201cun\u201d + \u201chappiness.\u201d  </li> <li>This is the most common approach in modern LLMs.  </li> </ul> </li> <li> <p>Character-Level Tokenization    Splits text into individual characters. Provides flexibility but creates very long sequences.  </p> </li> </ol>"},{"location":"tokenization/#4-tokenization-in-llms","title":"4. Tokenization in LLMs","text":"<p>Different models use different tokenizers:   - OpenAI GPT models: Use a variant of Byte Pair Encoding.   - Google models (e.g., BERT, T5): Often use WordPiece or SentencePiece.   - Meta\u2019s LLaMA models: Use SentencePiece with byte-fallback.  </p> <p>This means that the same text can result in different token counts depending on the model.  </p> <p>Example with GPT-style tokenizer:   - Text: \u201cChatGPT is amazing!\u201d   - Tokens: [\u201cChat\u201d, \u201cG\u201d, \u201cPT\u201d, \u201c is\u201d, \u201c amazing\u201d, \u201c!\u201d] \u2192 6 tokens.</p>"},{"location":"tokenization/#5-context-window","title":"5. Context Window","text":"<p>The context window is the maximum number of tokens a model can process at once.  </p> <ul> <li>GPT-3.5: ~4K tokens  </li> <li>GPT-4 Turbo: 128K tokens  </li> <li>Claude 3 Opus: 200K+ tokens  </li> </ul> <p>A larger context window allows feeding in longer documents, but comes with trade-offs in speed and cost.</p>"},{"location":"tokenization/#6-practical-aspects","title":"6. Practical Aspects","text":"<ul> <li>Counting Tokens: Tools like OpenAI\u2019s <code>tiktoken</code> or Hugging Face\u2019s <code>tokenizers</code> can measure token usage.  </li> <li>Chunking: For long documents, text is broken into chunks to fit within the context window.  </li> <li>Sliding Windows: Overlapping chunks ensure continuity in meaning.  </li> <li>Summarization: Long inputs can be shortened to fit context limits.  </li> </ul>"},{"location":"tokenization/#7-summary","title":"7. Summary","text":"<p>Tokenization defines how text is broken down for LLMs. It affects:</p> <ul> <li>Input representation  </li> <li>Context length  </li> <li>Cost of usage  </li> <li>Retrieval and RAG strategies  </li> </ul> <p>Mastering tokenization is essential for working with large-scale AI systems.</p>"},{"location":"tokenization/#further-reading","title":"Further Reading","text":"<ul> <li>The Illustrated Guide to Tokenizers (Hugging Face Course)  </li> <li>OpenAI tiktoken Library </li> <li>Byte Pair Encoding Explained </li> <li>SentencePiece: A Simple and Language Independent Subword Tokenizer </li> <li>How Tokenization Affects Context Windows </li> </ul>"},{"location":"transformers/","title":"Transformers","text":"<p>Transformers are a type of neural network architecture introduced in the paper Attention Is All You Need (Vaswani et al., 2017). They form the foundation of most modern large language models (LLMs), including GPT, BERT, and LLaMA.</p>"},{"location":"transformers/#why-transformers","title":"Why Transformers?","text":"<p>Traditional models like RNNs and LSTMs read sequences step by step, which makes training slow and limits their ability to handle long-range context.  </p> <p>Transformers use self-attention, allowing each token to consider relationships with all other tokens in the sequence, in parallel.</p> <p>Example: \u201cThe animal didn\u2019t cross the street because it was too tired.\u201d Self-attention links \u201cit\u201d \u2192 \u201canimal\u201d, enabling better understanding.</p>"},{"location":"transformers/#core-components","title":"Core Components","text":"<ol> <li>Embedding Layer \u2014 converts tokens into continuous vectors.  </li> <li>Positional Encoding \u2014 injects word order into embeddings.  </li> <li>Self-Attention \u2014 each word looks at others to find relevant context.  </li> <li>Multi-Head Attention \u2014 multiple attention \u201cheads\u201d in parallel for richer relationships.  </li> <li>Feed-Forward Networks \u2014 add non-linear transformations.  </li> <li>Residual Connections + Layer Norm \u2014 stabilize and speed up training.  </li> <li>Stacked Layers \u2014 repeating blocks build depth and power.  </li> </ol>"},{"location":"transformers/#encoder-vs-decoder","title":"Encoder vs Decoder","text":"<ul> <li>Encoder stack: processes input sequences (e.g., BERT).  </li> <li>Decoder stack: generates output sequences (e.g., GPT).  </li> <li>Encoder-Decoder: full translation pipeline (e.g., original Transformer, T5).  </li> </ul>"},{"location":"transformers/#visualization","title":"Visualization","text":""},{"location":"transformers/#example-sentiment-analyzer-python","title":"Example: Sentiment Analyzer (Python)","text":"<pre><code>from transformers import pipeline\nsentiment_analyzer = pipeline(\"sentiment-analysis\")\nprint(sentiment_analyzer(\"I love learning AI with Hugging face!\"))\nprint(sentiment_analyzer(\"This is th worst movie i have ever seen.\"))\n\noutput:\n[{'label': 'POSITIVE', 'score': 0.9997360110282898}]\n[{'label': 'NEGATIVE', 'score': 0.9997711777687073}]\n\nprint(sentiment_analyzer(\"This is cake tastes so good.\"))\noutput:\n[{'label': 'POSITIVE', 'score': 0.9998664855957031}]\n</code></pre>"},{"location":"transformers/#example-text-generator-python","title":"Example: Text Generator (Python)","text":"<pre><code>from transformers import pipeline\ngenerator = pipeline(\"text-generation\", model = \"gpt2\")\n# Generating Text\nresult = generator(\n    \"In the future of AI,\", # prompt\n    max_length=50, # max tokens to generate\n    num_return_sequences=1, # ho many ompletions to return\n    temperature=0.7 # randomness (lower = more focussed, higher = more creative)\n)\nprint(result[0][\"generated_text\"])\n</code></pre>"},{"location":"transformers/#why-transformers-matter","title":"Why Transformers Matter","text":"<ul> <li> <p>Parallelizable \u2192 much faster training than RNNs/LSTMs.</p> </li> <li> <p>Long-range dependencies \u2192 attention links distant words directly.</p> </li> <li> <p>Scalable \u2192 billions of parameters, foundation of modern AI.</p> </li> </ul>"},{"location":"transformers/#deep-dive","title":"Deep-Dive :","text":"<ul> <li>Attention Is All You Need (Vaswani et al., 2017)</li> <li>Illustrated Transformer (Jay Alammar)</li> <li>The Annotated Transformer (Harvard NLP)</li> </ul>"}]}