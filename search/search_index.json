{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-ai-learning-journey","title":"\ud83d\udc4b Welcome to the AI Learning Journey","text":"<p>This site captures my 5-day sprint into AI \u2014 from foundations to agents \u2014 so others can learn alongside me.</p>"},{"location":"#goals","title":"\ud83c\udf1f Goals","text":"<ul> <li>Build a solid foundation (tokenization, embeddings, transformers)</li> <li>Practice prompting techniques with hands-on examples</li> <li>Explore LLMs (open models &amp; hosted APIs)</li> <li>Understand agents &amp; multi-agents in workflows</li> <li>Share mini-projects as showcases</li> </ul>"},{"location":"#5-day-plan","title":"\ud83d\udcc5 5-Day Plan","text":"Day Focus Key Topics Day 1 Basics foundations, vectors, embeddings, tokenization Day 2 Prompting techniques, examples, anti-patterns Day 3 Transformers &amp; LLMs attention, fine-tuning, model choices Day 4 Agents tools, memory, workflows Day 5 Multi-Agents + Showcase coordination patterns, case studies, projects"},{"location":"#what-youll-find-here","title":"\ud83e\udded What you\u2019ll find here","text":"<ul> <li>Basics \u2192 foundations like tokenization, embeddings, and vectors  </li> <li>Prompting \u2192 techniques, examples, and pitfalls  </li> <li>Transformers \u2192 attention, training, fine-tuning  </li> <li>LLMs \u2192 open vs hosted models  </li> <li>Agents \u2192 tools, memory, workflows  </li> <li>Multi-Agents \u2192 collaboration patterns  </li> <li>Showcase \u2192 small projects &amp; demos built during this journey  </li> </ul> <p>How to use this site</p> <p>Browse day-by-day, or jump into a topic you\u2019re curious about. Each page has concept notes, code snippets, exercises, and takeaways.</p>"},{"location":"agents/","title":"Agents","text":""},{"location":"basics/","title":"Basics","text":"<p>This section provides foundational concepts that underlie large language models (LLMs) and their applications. It covers how models process text, how meaning can be represented numerically, and how text is broken into units for computation. Each topic includes references for deeper exploration.</p>"},{"location":"basics/#topics","title":"Topics","text":"<ul> <li>Foundations \u2192 overview of how LLMs operate as sequence predictors  </li> <li>Vectors &amp; Embeddings \u2192 representation of text as numerical vectors  </li> <li>Tokenization \u2192 conversion of raw text into model-readable tokens  </li> </ul> <p>For further background, see: - The Illustrated Transformer (Jay Alammar) - OpenAI: How GPT models work - Stanford CS324: Large Language Models</p>"},{"location":"basics/embeddings/","title":"Vectors &amp; Embeddings","text":"<p>An embedding is a numerical representation of text. Words, phrases, or documents are mapped to high-dimensional vectors such that items with similar meaning are located near each other in the vector space.</p>"},{"location":"basics/embeddings/#applications","title":"Applications","text":"<ul> <li>Semantic search \u2192 retrieving documents based on meaning rather than keywords  </li> <li>Clustering and classification \u2192 grouping related content automatically  </li> <li>Retrieval-augmented generation (RAG) \u2192 supplying relevant documents to LLMs during generation  </li> <li>Evaluation \u2192 comparing model outputs against references using similarity  </li> </ul>"},{"location":"basics/embeddings/#example-cosine-similarity","title":"Example: Cosine Similarity","text":"<p>The following code shows how to measure similarity between two vectors using cosine similarity. It measures the angle between two vectors \u2014 smaller angle = higher similarity.</p> <pre><code>def cosine(a, b):\n    import math\n    # Compute dot product of the two vectors\n    dot = sum(x*y for x, y in zip(a, b))\n\n    # Compute magnitude (length) of each vector\n    na = math.sqrt(sum(x*x for x in a))\n    nb = math.sqrt(sum(x*x for x in b))\n\n    # Divide dot product by product of magnitudes\n    return dot / (na * nb + 1e-12)  # small constant avoids division by zero\n\n# Example usage\nv1 = [1, 0, 1]\nv2 = [0.9, 0.1, 1.1]\nprint(round(cosine(v1, v2), 4))  # ~0.98 \u2192 very similar\n</code></pre>"},{"location":"basics/embeddings/#explanation","title":"Explanation:","text":"<p>Dot product \u2192 measures overlap between the vectors.</p> <p>Magnitudes \u2192 normalize for vector length (longer sentences \u2260 higher score).</p> <p>Cosine similarity \u2192 dot / (lengths). Range:</p> <ul> <li> <p>1.0 \u2192 identical direction (very similar meaning).</p> </li> <li> <p>0.0 \u2192 orthogonal (no relation).</p> </li> <li> <p>-1.0 \u2192 opposite meaning (rare in embeddings).</p> </li> </ul>"},{"location":"basics/embeddings/#why-this-matters","title":"Why this matters","text":"<p>Embeddings allow semantic tasks like:</p> <ul> <li> <p>\u201cFind me texts similar to this one.\u201d</p> </li> <li> <p>\u201cCluster related documents automatically.\u201d</p> </li> <li> <p>\u201cEvaluate if two answers mean the same thing.\u201d</p> </li> </ul>"},{"location":"basics/embeddings/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Text Embeddings (OpenAI docs)</p> </li> <li> <p>Sentence Transformers library</p> </li> <li> <p>Efficient Estimation of Word Representations (Mikolov et al., 2013)</p> </li> </ul>"},{"location":"basics/foundations/","title":"Foundations","text":"<p>Large language models are probabilistic sequence models. They are trained to predict the next token in a sequence given all previous tokens. Despite their complexity, their operation can be summarized as repeated prediction steps.</p>"},{"location":"basics/foundations/#core-mechanism","title":"Core mechanism","text":"<ol> <li>Input: a text prompt is provided by the user.  </li> <li>Tokenization: the text is divided into discrete units called tokens.  </li> <li>Processing: tokens are mapped to embeddings and processed by a transformer architecture.  </li> <li>Prediction: the model outputs a probability distribution over possible next tokens.  </li> <li>Decoding: a decoding strategy selects the next token.  </li> <li>Iteration: the process repeats until an end condition is met.  </li> </ol>"},{"location":"basics/foundations/#parameters-that-influence-output","title":"Parameters that influence output","text":"<ul> <li>Temperature \u2192 controls randomness; low values = deterministic, high = diverse  </li> <li>Top-k sampling \u2192 restricts choices to the k most likely tokens  </li> <li>Top-p (nucleus) sampling \u2192 samples from the smallest set of tokens with cumulative probability \u2265 p  </li> </ul>"},{"location":"basics/foundations/#minimal-pipeline","title":"Minimal pipeline","text":"<p>[Prompt text] \u2192 [Tokenizer] \u2192 [Token IDs] \u2192 [Transformer] \u2192 [Predicted IDs] \u2192 [Text output]</p>"},{"location":"basics/foundations/#example-sampling-in-python","title":"Example (sampling in Python)","text":"<p>The following code illustrates a simplified next-token sampling function.  It takes in a list of probabilities for each possible token and selects one,  using temperature scaling to control randomness.</p> <pre><code>import numpy as np\n\ndef sample_next_token(probabilities, temperature=0.7):\n    # Convert the list into a numpy array for math operations\n    p = np.array(probabilities, dtype=float)\n\n    # Apply temperature: lower temperature sharpens the distribution,\n    # higher temperature flattens it (more randomness)\n    p = np.log(p + 1e-9) / max(temperature, 1e-6)\n\n    # Exponentiate and normalize so probabilities sum to 1\n    p = np.exp(p) / np.exp(p).sum()\n\n    # Randomly select a token index based on adjusted probabilities\n    return np.random.choice(len(p), p=p)\n</code></pre> <p>Explanation</p> <p>Input: A list of probabilities, e.g. [0.1, 0.7, 0.2].</p> <p>Temperature scaling: Adjusts how \u201cpeaked\u201d or \u201cflat\u201d the distribution is.</p> <p>temperature=0.1 \u2192 Almost always the top token.</p> <p>temperature=1.0 \u2192 Original distribution.</p> <p>temperature=2.0 \u2192 More randomness.</p> <p>Normalization : Ensures probabilities add up to 1 after scaling.</p> <p>Sampling : Randomly picks one token according to the adjusted distribution.</p> <p>Why this matters: </p> <ul> <li>LLMs do not \u201cchoose\u201d deterministically unless you set temperature=0.</li> <li>Understanding sampling explains why two runs of the same prompt can produce slightly different outputs.</li> </ul> <p>Further Reading</p> <ul> <li>The transformer architecture was introduced in Attention Is All You Need.</li> <li>A practical overview is in Hugging Face docs.</li> </ul>"},{"location":"basics/tokenization/","title":"Tokenization","text":"<p>Language models do not process raw text directly. Instead, they operate on discrete units called tokens. A token may represent a whole word, a subword, or even a single character depending on the tokenizer. Understanding tokenization is essential for controlling cost, managing context limits, and designing effective prompts.</p>"},{"location":"basics/tokenization/#why-tokenization-matters","title":"Why Tokenization Matters","text":"<ul> <li>Cost &amp; efficiency \u2192 most APIs bill per token (both input + output).  </li> <li>Context limits \u2192 each model has a maximum context window (e.g., 4k, 16k, 128k tokens).  </li> <li>Output control \u2192 concise prompts = fewer tokens, lower cost, and more predictable outputs.  </li> </ul>"},{"location":"basics/tokenization/#example-approximate-token-counting","title":"Example: Approximate Token Counting","text":"<p>A rough rule of thumb: 1 token \u2248 4 characters of English text. This simple function provides a quick estimate:</p> <pre><code>def rough_token_count(text: str) -&gt; int:\n    \"\"\"\n    Quick, language-dependent approximation:\n    ~1 token per 4 characters of English text.\n    Use model-specific tokenizers for production accuracy.\n    \"\"\"\n    return max(1, len(text) // 4)\n\n# Example\nprint(rough_token_count(\"Hello, world!\"))  # ~3\n</code></pre>"},{"location":"basics/tokenization/#explanation","title":"Explanation","text":"<p>1) Divide string length by 4 \u2192 gives an approximate token count.</p> <p>2) max(1, \u2026) ensures even short strings return \u22651 token.</p> <p>3) Useful for early estimation, but not reliable for non-English text, emojis, or special characters.</p>"},{"location":"basics/tokenization/#accurate-token-counting-model-specific","title":"Accurate Token Counting (Model-Specific)","text":"<p>For precise counts, always use the tokenizer that matches your model.</p> <ul> <li> <p>OpenAI GPT models \u2192 tiktoken</p> </li> <li> <p>Many open models (e.g., LLaMA, T5) \u2192 SentencePiece</p> </li> <li> <p>General concept \u2192 Byte Pair Encoding (BPE)</p> </li> </ul> <pre><code># Exact token counting with OpenAI's GPT tokenizer\n# Install with: pip install tiktoken\nimport tiktoken\n\nenc = tiktoken.get_encoding(\"cl100k_base\")  # encoding for GPT-4\ntext = \"Hello, world!\"\ntokens = enc.encode(text)\n\nprint(tokens)      # [9906, 11, 1917, 0]\nprint(len(tokens)) # exact token count (4 here)\n</code></pre>"},{"location":"basics/tokenization/#explanation_1","title":"Explanation","text":"<p>1) get_encoding(\"cl100k_base\") loads the tokenizer used by GPT-4.</p> <p>2) encode(text) converts text \u2192 list of token IDs.</p> <p>3) len(tokens) gives the exact token count.</p> <p>4) Results vary by model \u2192 always use the tokenizer for the model you plan to run.</p>"},{"location":"basics/tokenization/#practical-guidelines","title":"Practical Guidelines","text":"<ul> <li> <p>Always budget prompts in tokens, not characters.</p> </li> <li> <p>Place the instruction first and keep it concise.</p> </li> <li> <p>Specify output format (e.g., JSON, bullet points) to compress meaning into fewer tokens.</p> </li> <li> <p>For long docs, chunk into 300\u2013500 tokens with overlap, then re-rank to feed only the most relevant chunks.</p> </li> <li> <p>Limit max_tokens in outputs to control cost and latency.</p> </li> </ul> <p>Fast sanity checks</p> <ul> <li>Use the model vendor\u2019s online tokenizer tool for quick checks.</li> <li>Keep a utility function in your repo to measure token counts before sending requests</li> </ul>"},{"location":"basics/tokenization/#further-reading","title":"Further Reading","text":"<ul> <li> <p>OpenAI Tokenizer Tool</p> </li> <li> <p>Byte Pair Encoding (Wikipedia)</p> </li> <li> <p>SentencePiece (Google GitHub)</p> </li> </ul>"},{"location":"llms/","title":"LLMs","text":""},{"location":"multi-agents/","title":"Multi-Agents","text":""},{"location":"prompting/","title":"Prompting","text":""},{"location":"showcase/","title":"Showcase","text":""},{"location":"transformers/","title":"Transformers","text":""}]}