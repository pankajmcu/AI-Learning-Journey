{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-ai-learning-journey","title":"\ud83d\udc4b Welcome to the AI Learning Journey","text":"<p>This site captures my 5-day sprint into AI \u2014 from foundations to agents \u2014 so others can learn alongside me.</p>"},{"location":"#goals","title":"\ud83c\udf1f Goals","text":"<ul> <li>Build a solid foundation (tokenization, embeddings, transformers)</li> <li>Practice prompting techniques with hands-on examples</li> <li>Explore LLMs (open models &amp; hosted APIs)</li> <li>Understand agents &amp; multi-agents in workflows</li> <li>Share mini-projects as showcases</li> </ul>"},{"location":"#5-day-plan","title":"\ud83d\udcc5 5-Day Plan","text":"Day Focus Key Topics Day 1 Basics foundations, vectors, embeddings, tokenization Day 2 Prompting techniques, examples, anti-patterns Day 3 Transformers &amp; LLMs attention, fine-tuning, model choices Day 4 Agents tools, memory, workflows Day 5 Multi-Agents + Showcase coordination patterns, case studies, projects"},{"location":"#what-youll-find-here","title":"\ud83e\udded What you\u2019ll find here","text":"<ul> <li>Basics \u2192 foundations like tokenization, embeddings, and vectors  </li> <li>Prompting \u2192 techniques, examples, and pitfalls  </li> <li>Transformers \u2192 attention, training, fine-tuning  </li> <li>LLMs \u2192 open vs hosted models  </li> <li>Agents \u2192 tools, memory, workflows  </li> <li>Multi-Agents \u2192 collaboration patterns  </li> <li>Showcase \u2192 small projects &amp; demos built during this journey  </li> </ul> <p>How to use this site</p> <p>Browse day-by-day, or jump into a topic you\u2019re curious about. Each page has concept notes, code snippets, exercises, and takeaways.</p>"},{"location":"agents/","title":"Agents","text":"<p>Agents extend Large Language Models (LLMs) by giving them the ability to use tools, plan tasks, and remember context. They are central to building AI systems that interact with the real world.</p>"},{"location":"agents/#core-building-blocks","title":"Core Building Blocks","text":"<ul> <li>LLM (Brain) \u2192 interprets instructions and reasons about tasks  </li> <li>Tools (Hands) \u2192 external APIs, calculators, databases, search engines  </li> <li>Memory \u2192 short-term or long-term storage of information  </li> <li>Planner \u2192 decides the sequence of steps  </li> <li>Executor \u2192 carries out the actions and returns results  </li> </ul>"},{"location":"agents/#example-workflow","title":"Example Workflow","text":"<p>Prompt: \u201cFind the population of France, then calculate 10% of it.\u201d</p> <p>Agent process:</p> <ol> <li>Plan: Identify steps \u2192 (a) search for population, (b) calculate 10%.  </li> <li>Tool use: Search API \u2192 returns \u201c67 million.\u201d  </li> <li>Reasoning: 10% of 67 million = 6.7 million.  </li> <li>Answer: <pre><code>10% of France\u2019s population \u2248 6.7 million\n</code></pre></li> </ol>"},{"location":"agents/#why-use-agents","title":"Why Use Agents?","text":"<p>LLMs alone cannot do accurate math, access the web, or update knowledge. Agents combine reasoning with external actions, making them more useful in practice. Applications: assistants, research copilots, robotic process automation (RPA).</p>"},{"location":"agents/#types-of-agents","title":"Types of Agents","text":"<ul> <li>Single-agent \u2192 one model with tools</li> <li>Multi-agent systems \u2192 multiple agents collaborating</li> <li>Specialized agents \u2192 focused on specific tasks (e.g., SQL agent, travel agent)</li> </ul>"},{"location":"agents/#challenges","title":"Challenges","text":"<ul> <li>Error propagation \u2192 mistakes in one step can affect the final result</li> <li>Latency \u2192 multiple tool calls can slow down responses</li> <li>Security \u2192 tool use may involve external risks</li> <li>Control \u2192 preventing infinite loops or unintended actions</li> </ul>"},{"location":"agents/#further-reads","title":"Further Reads","text":"<p>ReAct: Reason + Act</p> <p>LangChain Agents Documentation</p> <p>AutoGPT (GitHub)</p>"},{"location":"basics/","title":"Basics","text":"<p>This section provides foundational concepts that underlie large language models (LLMs) and their applications. It covers how models process text, how meaning can be represented numerically, and how text is broken into units for computation. Each topic includes references for deeper exploration.</p>"},{"location":"basics/#topics","title":"Topics","text":"<ul> <li>Foundations \u2192 overview of how LLMs operate as sequence predictors  </li> <li>Vectors &amp; Embeddings \u2192 representation of text as numerical vectors  </li> <li>Tokenization \u2192 conversion of raw text into model-readable tokens  </li> </ul> <p>For further background, see: - The Illustrated Transformer (Jay Alammar) - OpenAI: How GPT models work - Stanford CS324: Large Language Models</p>"},{"location":"basics/embeddings/","title":"Vectors &amp; Embeddings","text":"<p>An embedding is a numerical representation of text. Words, phrases, or documents are mapped to high-dimensional vectors such that items with similar meaning are located near each other in the vector space.</p>"},{"location":"basics/embeddings/#applications","title":"Applications","text":"<ul> <li>Semantic search \u2192 retrieving documents based on meaning rather than keywords  </li> <li>Clustering and classification \u2192 grouping related content automatically  </li> <li>Retrieval-augmented generation (RAG) \u2192 supplying relevant documents to LLMs during generation  </li> <li>Evaluation \u2192 comparing model outputs against references using similarity  </li> </ul>"},{"location":"basics/embeddings/#example-cosine-similarity","title":"Example: Cosine Similarity","text":"<p>The following code shows how to measure similarity between two vectors using cosine similarity. It measures the angle between two vectors \u2014 smaller angle = higher similarity.</p> <pre><code>def cosine(a, b):\n    import math\n    # Compute dot product of the two vectors\n    dot = sum(x*y for x, y in zip(a, b))\n\n    # Compute magnitude (length) of each vector\n    na = math.sqrt(sum(x*x for x in a))\n    nb = math.sqrt(sum(x*x for x in b))\n\n    # Divide dot product by product of magnitudes\n    return dot / (na * nb + 1e-12)  # small constant avoids division by zero\n\n# Example usage\nv1 = [1, 0, 1]\nv2 = [0.9, 0.1, 1.1]\nprint(round(cosine(v1, v2), 4))  # ~0.98 \u2192 very similar\n</code></pre>"},{"location":"basics/embeddings/#explanation","title":"Explanation:","text":"<p>Dot product \u2192 measures overlap between the vectors.</p> <p>Magnitudes \u2192 normalize for vector length (longer sentences \u2260 higher score).</p> <p>Cosine similarity \u2192 dot / (lengths). Range:</p> <ul> <li> <p>1.0 \u2192 identical direction (very similar meaning).</p> </li> <li> <p>0.0 \u2192 orthogonal (no relation).</p> </li> <li> <p>-1.0 \u2192 opposite meaning (rare in embeddings).</p> </li> </ul>"},{"location":"basics/embeddings/#why-this-matters","title":"Why this matters","text":"<p>Embeddings allow semantic tasks like:</p> <ul> <li> <p>\u201cFind me texts similar to this one.\u201d</p> </li> <li> <p>\u201cCluster related documents automatically.\u201d</p> </li> <li> <p>\u201cEvaluate if two answers mean the same thing.\u201d</p> </li> </ul>"},{"location":"basics/embeddings/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Text Embeddings (OpenAI docs)</p> </li> <li> <p>Sentence Transformers library</p> </li> <li> <p>Efficient Estimation of Word Representations (Mikolov et al., 2013)</p> </li> </ul>"},{"location":"basics/foundations/","title":"Foundations","text":"<p>Large language models are probabilistic sequence models. They are trained to predict the next token in a sequence given all previous tokens. Despite their complexity, their operation can be summarized as repeated prediction steps.</p>"},{"location":"basics/foundations/#core-mechanism","title":"Core mechanism","text":"<ol> <li>Input: a text prompt is provided by the user.  </li> <li>Tokenization: the text is divided into discrete units called tokens.  </li> <li>Processing: tokens are mapped to embeddings and processed by a transformer architecture.  </li> <li>Prediction: the model outputs a probability distribution over possible next tokens.  </li> <li>Decoding: a decoding strategy selects the next token.  </li> <li>Iteration: the process repeats until an end condition is met.  </li> </ol>"},{"location":"basics/foundations/#parameters-that-influence-output","title":"Parameters that influence output","text":"<ul> <li>Temperature \u2192 controls randomness; low values = deterministic, high = diverse  </li> <li>Top-k sampling \u2192 restricts choices to the k most likely tokens  </li> <li>Top-p (nucleus) sampling \u2192 samples from the smallest set of tokens with cumulative probability \u2265 p  </li> </ul>"},{"location":"basics/foundations/#minimal-pipeline","title":"Minimal pipeline","text":"<p>[Prompt text] \u2192 [Tokenizer] \u2192 [Token IDs] \u2192 [Transformer] \u2192 [Predicted IDs] \u2192 [Text output]</p>"},{"location":"basics/foundations/#example-sampling-in-python","title":"Example (sampling in Python)","text":"<p>The following code illustrates a simplified next-token sampling function.  It takes in a list of probabilities for each possible token and selects one,  using temperature scaling to control randomness.</p> <pre><code>import numpy as np\n\ndef sample_next_token(probabilities, temperature=0.7):\n    # Convert the list into a numpy array for math operations\n    p = np.array(probabilities, dtype=float)\n\n    # Apply temperature: lower temperature sharpens the distribution,\n    # higher temperature flattens it (more randomness)\n    p = np.log(p + 1e-9) / max(temperature, 1e-6)\n\n    # Exponentiate and normalize so probabilities sum to 1\n    p = np.exp(p) / np.exp(p).sum()\n\n    # Randomly select a token index based on adjusted probabilities\n    return np.random.choice(len(p), p=p)\n</code></pre> <p>Explanation</p> <p>Input: A list of probabilities, e.g. [0.1, 0.7, 0.2].</p> <p>Temperature scaling: Adjusts how \u201cpeaked\u201d or \u201cflat\u201d the distribution is.</p> <p>temperature=0.1 \u2192 Almost always the top token.</p> <p>temperature=1.0 \u2192 Original distribution.</p> <p>temperature=2.0 \u2192 More randomness.</p> <p>Normalization : Ensures probabilities add up to 1 after scaling.</p> <p>Sampling : Randomly picks one token according to the adjusted distribution.</p> <p>Why this matters: </p> <ul> <li>LLMs do not \u201cchoose\u201d deterministically unless you set temperature=0.</li> <li>Understanding sampling explains why two runs of the same prompt can produce slightly different outputs.</li> </ul> <p>Further Reading</p> <ul> <li>The transformer architecture was introduced in Attention Is All You Need.</li> <li>A practical overview is in Hugging Face docs.</li> </ul>"},{"location":"basics/tokenization/","title":"Tokenization","text":"<p>Language models do not process raw text directly. Instead, they operate on discrete units called tokens. A token may represent a whole word, a subword, or even a single character depending on the tokenizer. Understanding tokenization is essential for controlling cost, managing context limits, and designing effective prompts.</p>"},{"location":"basics/tokenization/#why-tokenization-matters","title":"Why Tokenization Matters","text":"<ul> <li>Cost &amp; efficiency \u2192 most APIs bill per token (both input + output).  </li> <li>Context limits \u2192 each model has a maximum context window (e.g., 4k, 16k, 128k tokens).  </li> <li>Output control \u2192 concise prompts = fewer tokens, lower cost, and more predictable outputs.  </li> </ul>"},{"location":"basics/tokenization/#example-approximate-token-counting","title":"Example: Approximate Token Counting","text":"<p>A rough rule of thumb: 1 token \u2248 4 characters of English text. This simple function provides a quick estimate:</p> <pre><code>def rough_token_count(text: str) -&gt; int:\n    \"\"\"\n    Quick, language-dependent approximation:\n    ~1 token per 4 characters of English text.\n    Use model-specific tokenizers for production accuracy.\n    \"\"\"\n    return max(1, len(text) // 4)\n\n# Example\nprint(rough_token_count(\"Hello, world!\"))  # ~3\n</code></pre>"},{"location":"basics/tokenization/#explanation","title":"Explanation","text":"<p>1) Divide string length by 4 \u2192 gives an approximate token count.</p> <p>2) max(1, \u2026) ensures even short strings return \u22651 token.</p> <p>3) Useful for early estimation, but not reliable for non-English text, emojis, or special characters.</p>"},{"location":"basics/tokenization/#accurate-token-counting-model-specific","title":"Accurate Token Counting (Model-Specific)","text":"<p>For precise counts, always use the tokenizer that matches your model.</p> <ul> <li> <p>OpenAI GPT models \u2192 tiktoken</p> </li> <li> <p>Many open models (e.g., LLaMA, T5) \u2192 SentencePiece</p> </li> <li> <p>General concept \u2192 Byte Pair Encoding (BPE)</p> </li> </ul> <pre><code># Exact token counting with OpenAI's GPT tokenizer\n# Install with: pip install tiktoken\nimport tiktoken\n\nenc = tiktoken.get_encoding(\"cl100k_base\")  # encoding for GPT-4\ntext = \"Hello, world!\"\ntokens = enc.encode(text)\n\nprint(tokens)      # [9906, 11, 1917, 0]\nprint(len(tokens)) # exact token count (4 here)\n</code></pre>"},{"location":"basics/tokenization/#explanation_1","title":"Explanation","text":"<p>1) get_encoding(\"cl100k_base\") loads the tokenizer used by GPT-4.</p> <p>2) encode(text) converts text \u2192 list of token IDs.</p> <p>3) len(tokens) gives the exact token count.</p> <p>4) Results vary by model \u2192 always use the tokenizer for the model you plan to run.</p>"},{"location":"basics/tokenization/#practical-guidelines","title":"Practical Guidelines","text":"<ul> <li> <p>Always budget prompts in tokens, not characters.</p> </li> <li> <p>Place the instruction first and keep it concise.</p> </li> <li> <p>Specify output format (e.g., JSON, bullet points) to compress meaning into fewer tokens.</p> </li> <li> <p>For long docs, chunk into 300\u2013500 tokens with overlap, then re-rank to feed only the most relevant chunks.</p> </li> <li> <p>Limit max_tokens in outputs to control cost and latency.</p> </li> </ul> <p>Fast sanity checks</p> <ul> <li>Use the model vendor\u2019s online tokenizer tool for quick checks.</li> <li>Keep a utility function in your repo to measure token counts before sending requests</li> </ul>"},{"location":"basics/tokenization/#further-reading","title":"Further Reading","text":"<ul> <li> <p>OpenAI Tokenizer Tool</p> </li> <li> <p>Byte Pair Encoding (Wikipedia)</p> </li> <li> <p>SentencePiece (Google GitHub)</p> </li> </ul>"},{"location":"llms/","title":"Large Language Models (LLMs)","text":"<p>Large Language Models (LLMs) are neural networks based on the Transformer architecture, trained on massive text datasets to predict the next token in a sequence. They form the backbone of modern AI systems, powering applications from chatbots to code assistants.</p>"},{"location":"llms/#visualization","title":"Visualization","text":"<p>The diagram below summarizes high level architecural flow in Large Language Models (LLMs).</p> <p></p>"},{"location":"llms/#what-it-shows","title":"What it shows","text":"<p>Training (left)</p> <ol> <li>Training data (text corpus) \u2014 books, websites, code, papers.</li> <li>Tokenization \u2014 text \u2192 tokens (e.g., <code>\"Hello world!\" \u2192 [15496, 995]</code>).</li> <li>Transformer layers \u2014 stacked blocks of self-attention + feed-forward networks learn patterns.</li> <li>Trained model \u2014 learned weights are saved for later use.</li> </ol> <p>Inference (right)</p> <ol> <li>Input prompt \u2014 e.g., \u201cWhat is the capital of France?\u201d</li> <li>Tokenization \u2014 prompt \u2192 tokens.</li> <li>Model \u2014 forward pass through the same transformer stack (no training).</li> <li>Output tokens \u2192 text \u2014 tokens decode to an answer (e.g., \u201cParis\u201d).</li> </ol> <p>Key idea:  Training learns statistical patterns from massive data; inference applies those patterns to new prompts.</p>"},{"location":"llms/#types-of-llms","title":"Types of LLMs","text":"<ul> <li>Open models \u2192 Downloadable and runnable locally  <ul> <li>Examples: LLaMA, Mistral, Falcon, Gemma  </li> </ul> </li> <li>Hosted APIs \u2192 Accessible via cloud services  <ul> <li>Examples: GPT (OpenAI), Claude (Anthropic), Gemini (Google)  </li> </ul> </li> <li>Domain-specific models \u2192 Trained for specialized use cases  <ul> <li>Examples: BioGPT (medical), StarCoder (code)  </li> </ul> </li> </ul>"},{"location":"llms/#capabilities","title":"Capabilities","text":"<ul> <li>Text generation: summaries, stories, explanations  </li> <li>Question answering: knowledge-based responses  </li> <li>Classification: spam detection, sentiment analysis  </li> <li>Translation: cross-lingual tasks  </li> <li>Reasoning: math, logic, planning  </li> <li>Tool use: via prompts or agent frameworks  </li> </ul>"},{"location":"llms/#limitations","title":"Limitations","text":"<ul> <li>Hallucination \u2192 May produce factually incorrect outputs  </li> <li>Bias \u2192 Reflects societal and dataset biases  </li> <li>Context window limits \u2192 Only process a fixed number of tokens at once  </li> <li>Knowledge staleness \u2192 Models only know up to their training cut-off  </li> </ul>"},{"location":"llms/#scale-and-parameters","title":"Scale and Parameters","text":"<ul> <li>LLMs are characterized by their parameter count:  <ul> <li>Small (1B\u20137B) \u2192 Lightweight, runs on laptops/GPUs  </li> <li>Medium (13B\u201334B) \u2192 More capable, needs good hardware  </li> <li>Large (65B\u2013540B) \u2192 State-of-the-art, requires distributed clusters  </li> </ul> </li> </ul> <p>Trade-off: larger = more accurate and capable, but slower and more costly.</p>"},{"location":"llms/#deployment-modes","title":"Deployment Modes","text":"<ul> <li>Cloud APIs \u2192 Simple setup, scalable, but data leaves your environment  </li> <li>On-prem / local \u2192 Privacy control, but high compute cost  </li> <li>Hybrid (RAG) \u2192 Retrieval-augmented generation extends smaller models with external knowledge  </li> </ul>"},{"location":"llms/#references","title":"References","text":"<ul> <li>A Survey of Large Language Models (Zhao et al., 2023) </li> <li>Hugging Face Model Hub </li> <li>Stanford HELM Benchmark </li> </ul>"},{"location":"multi-agents/","title":"Multi-Agents","text":""},{"location":"prompting/","title":"Prompting","text":"<p>Prompting is the practice of instructing a large language model (LLM) to perform a task. The structure and clarity of the prompt strongly influence the quality of the response.</p>"},{"location":"prompting/#core-principles","title":"Core Principles","text":"<ol> <li> <p>Role \u2014 define the model\u2019s persona  </p> <p>\u201cYou are a helpful Python tutor.\u201d </p> </li> <li> <p>Goal \u2014 specify the exact task  </p> <p>\u201cExplain decorators in simple terms.\u201d </p> </li> <li> <p>Constraints \u2014 add rules  </p> <p>\u201cUse bullet points, max 5 lines.\u201d </p> </li> <li> <p>Format \u2014 define output structure  </p> <p>\u201cReturn JSON with keys: title, summary.\u201d </p> </li> <li> <p>Examples (few-shot) \u2014 show inputs &amp; outputs  </p> <pre><code>English: Hello \u2192 French: Bonjour  \nEnglish: Thank you \u2192 French: Merci  \nEnglish: Good night \u2192 French:\n</code></pre> </li> </ol>"},{"location":"prompting/#prompting-techniques","title":"Prompting Techniques","text":"<ul> <li> <p>Zero-shot \u2192 Direct instruction without examples Prompt: <pre><code>Translate \"machine learning\" into Spanish.\n</code></pre> Output: <pre><code>aprendizaje autom\u00e1tico\n</code></pre></p> </li> <li> <p>Few-shot \u2192 Include examples to guide behavior Prompt: <pre><code>English: Hello \u2192 French: Bonjour\nEnglish: Thank you \u2192 French: Merci\nEnglish: Good night \u2192 French:\n</code></pre> Output: <pre><code>Bonne nuit\n</code></pre></p> </li> <li>Chain of Thought (CoT) \u2192 Ask for step-by-step reasoning Prompt: <pre><code>Solve step by step:\nA pen costs $1 and a notebook costs $2. \nIf I buy 3 pens and 2 notebooks, how much do I spend in total?\n</code></pre> Output: <pre><code>Step 1: Cost of 3 pens = 3 \u00d7 $1 = $3\nStep 2: Cost of 2 notebooks = 2 \u00d7 $2 = $4\nStep 3: Total = $3 + $4 = $7\nFinal Answer: $7\n</code></pre></li> <li>ReAct \u2192 Reasoning combined with external actions/tools  Prompt: <pre><code>Task: Find out who won the 2018 FIFA World Cup and explain in one sentence.\nPlan: \n1. Search for the answer.\n2. Return a short explanation.\n</code></pre> Output: <pre><code>The 2018 FIFA World Cup was won by France, who defeated Croatia 4\u20132 in the final.\n</code></pre></li> <li>Self-critique \u2192 generate, evaluate, and refine outputs Prompt: <pre><code>Write a two-sentence summary of what transformers are. \nThen critique the summary. \nFinally, provide an improved version.\n</code></pre> Output: <pre><code>Initial Summary:\nTransformers are deep learning models that use attention to process sequences. \nThey are widely used for NLP tasks.\n\nCritique:\nThe explanation is too brief and lacks context about their significance.\n\nImproved Summary:\nTransformers are neural network architectures built around self-attention, \nallowing them to model long-range dependencies in text efficiently. \nThey form the foundation of modern large language models used in NLP and beyond.\n</code></pre></li> </ul>"},{"location":"prompting/#anti-patterns","title":"Anti-Patterns","text":"<ul> <li>Vague requests (\u201cExplain AI.\u201d)  </li> <li>Long prompts with key info buried at the end  </li> <li>Implicit requirements not stated clearly  </li> <li>Missing or inconsistent output format  </li> </ul>"},{"location":"prompting/#why-prompting-matters","title":"Why Prompting Matters","text":"<ul> <li>Saves tokens \u2192 lower cost &amp; faster responses</li> <li>Produces predictable outputs</li> <li>Enables automation and evaluation when outputs are structured</li> </ul>"},{"location":"prompting/#deep-dive","title":"Deep Dive","text":"<ul> <li>Prompt Engineering Guide</li> <li>OpenAI Cookbook</li> <li>Chain of Thought Reasoning (Wei et al., 2022)</li> </ul>"},{"location":"rag/","title":"Retrieval-Augmented Generation (RAG)","text":"<p>Retrieval-Augmented Generation (RAG) is a method to extend Large Language Models (LLMs) with external knowledge sources. Instead of relying only on pre-trained data, RAG retrieves relevant information at runtime and injects it into the model\u2019s context.</p>"},{"location":"rag/#why-rag","title":"Why RAG?","text":"<ul> <li>LLMs may hallucinate when knowledge is missing.  </li> <li>Training or fine-tuning large models is expensive.  </li> <li>RAG provides fresh, domain-specific knowledge on demand.  </li> </ul> <p>Example: \u201cSummarize our company\u2019s Q1 financial report.\u201d - Plain LLM \u2192 may hallucinate. - RAG \u2192 retrieves the actual report \u2192 summary is grounded in real data.  </p>"},{"location":"rag/#workflow","title":"Workflow","text":"<ol> <li>Chunk documents \u2192 Split into manageable passages (e.g., 300\u2013500 tokens).  </li> <li>Embed chunks \u2192 Convert text into vectors using an embedding model.  </li> <li>Store vectors \u2192 Save in a vector database (FAISS, PGVector, Pinecone, Weaviate).  </li> <li>User query \u2192 Embed the question into the same vector space.  </li> <li>Retrieve \u2192 Find the most similar chunks using cosine similarity.  </li> <li>Augment prompt \u2192 Attach the retrieved chunks as context.  </li> <li>Generate \u2192 LLM uses both the prompt and retrieved context to produce an answer.  </li> </ol>"},{"location":"rag/#example-prompt","title":"Example Prompt","text":"<pre><code>You are a helpful assistant. \nUse only the provided context to answer the question.\n\n# Question:\nWho is the CEO of OpenAI?\n\n# Context:\n1. OpenAI Leadership (2023): Sam Altman is the CEO.\n2. OpenAI Board Notes: The company is based in San Francisco.\n\n# Instructions:\n- If the answer is not in the context, say \u201cI don\u2019t know.\u201d\n- Be concise.\n</code></pre>"},{"location":"rag/#strengths","title":"Strengths","text":"<ul> <li>Reduces hallucination.</li> <li>Provides fresh and updated knowledge.</li> <li>Allows smaller models to behave like larger ones.</li> </ul>"},{"location":"rag/#challenges","title":"Challenges","text":"<ul> <li>Chunking strategy affects retrieval quality.</li> <li>Embedding quality determines similarity search performance.</li> <li>Context window limits restrict how much context can be passed.</li> <li>Latency from vector search can add overhead.</li> </ul>"},{"location":"rag/#further-reads","title":"Further Reads","text":"<p>Retrieval-Augmented Generation (Lewis et al., 2020)</p> <p>Pinecone: What is RAG?</p> <p>Hugging Face: RAG Models</p>"},{"location":"showcase/","title":"Showcase","text":""},{"location":"showcase/#rag-mini-bm25-vs-embeddings","title":"RAG Mini \u2014 BM25 vs Embeddings","text":"<p>Try it live below (embedded from Hugging Face Spaces):</p> <p> </p> <p>If the embedded app doesn\u2019t load, open it directly: https://huggingface.co/spaces/pantji/pankaj_rag-mini</p>"},{"location":"transformers/","title":"Transformers","text":"<p>Transformers are a type of neural network architecture introduced in the paper Attention Is All You Need (Vaswani et al., 2017). They form the foundation of most modern large language models (LLMs), including GPT, BERT, and LLaMA.</p>"},{"location":"transformers/#why-transformers","title":"Why Transformers?","text":"<p>Traditional models like RNNs and LSTMs read sequences step by step, which makes training slow and limits their ability to handle long-range context.  </p> <p>Transformers use self-attention, allowing each token to consider relationships with all other tokens in the sequence, in parallel.</p> <p>Example: \u201cThe animal didn\u2019t cross the street because it was too tired.\u201d Self-attention links \u201cit\u201d \u2192 \u201canimal\u201d, enabling better understanding.</p>"},{"location":"transformers/#core-components","title":"Core Components","text":"<ol> <li>Embedding Layer \u2014 converts tokens into continuous vectors.  </li> <li>Positional Encoding \u2014 injects word order into embeddings.  </li> <li>Self-Attention \u2014 each word looks at others to find relevant context.  </li> <li>Multi-Head Attention \u2014 multiple attention \u201cheads\u201d in parallel for richer relationships.  </li> <li>Feed-Forward Networks \u2014 add non-linear transformations.  </li> <li>Residual Connections + Layer Norm \u2014 stabilize and speed up training.  </li> <li>Stacked Layers \u2014 repeating blocks build depth and power.  </li> </ol>"},{"location":"transformers/#encoder-vs-decoder","title":"Encoder vs Decoder","text":"<ul> <li>Encoder stack: processes input sequences (e.g., BERT).  </li> <li>Decoder stack: generates output sequences (e.g., GPT).  </li> <li>Encoder-Decoder: full translation pipeline (e.g., original Transformer, T5).  </li> </ul>"},{"location":"transformers/#visualization","title":"Visualization","text":""},{"location":"transformers/#example-sentiment-analyzer-python","title":"Example: Sentiment Analyzer (Python)","text":"<pre><code>from transformers import pipeline\nsentiment_analyzer = pipeline(\"sentiment-analysis\")\nprint(sentiment_analyzer(\"I love learning AI with Hugging face!\"))\nprint(sentiment_analyzer(\"This is th worst movie i have ever seen.\"))\n\noutput:\n[{'label': 'POSITIVE', 'score': 0.9997360110282898}]\n[{'label': 'NEGATIVE', 'score': 0.9997711777687073}]\n\nprint(sentiment_analyzer(\"This is cake tastes so good.\"))\noutput:\n[{'label': 'POSITIVE', 'score': 0.9998664855957031}]\n</code></pre>"},{"location":"transformers/#example-text-generator-python","title":"Example: Text Generator (Python)","text":"<pre><code>from transformers import pipeline\ngenerator = pipeline(\"text-generation\", model = \"gpt2\")\n# Generating Text\nresult = generator(\n    \"In the future of AI,\", # prompt\n    max_length=50, # max tokens to generate\n    num_return_sequences=1, # ho many ompletions to return\n    temperature=0.7 # randomness (lower = more focussed, higher = more creative)\n)\nprint(result[0][\"generated_text\"])\n</code></pre>"},{"location":"transformers/#why-transformers-matter","title":"Why Transformers Matter","text":"<ul> <li> <p>Parallelizable \u2192 much faster training than RNNs/LSTMs.</p> </li> <li> <p>Long-range dependencies \u2192 attention links distant words directly.</p> </li> <li> <p>Scalable \u2192 billions of parameters, foundation of modern AI.</p> </li> </ul>"},{"location":"transformers/#deep-dive","title":"Deep-Dive :","text":"<ul> <li>Attention Is All You Need (Vaswani et al., 2017)</li> <li>Illustrated Transformer (Jay Alammar)</li> <li>The Annotated Transformer (Harvard NLP)</li> </ul>"}]}