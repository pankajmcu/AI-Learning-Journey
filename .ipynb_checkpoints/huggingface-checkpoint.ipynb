{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ade1fa1-eb3c-4778-b930-269ce3a3e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997360110282898}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997711777687073}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "print(sentiment_analyzer(\"I love learning AI with Hugging face!\"))\n",
    "print(sentiment_analyzer(\"This is th worst movie i have ever seen.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ceba9a8-c473-4200-a434-34b20a7862f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998664855957031}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analyzer(\"This is cake tastes so good.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53510721-b4a5-4f18-b809-6c5462d54314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0df7444c0564a2bb75f67cbfef1d4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AI-Learning-Journey\\aienv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4503fabc292443ed9b9e496196de9928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e312e38c10344eefb22733baea96b200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae94951bb3f849cf8c9def18e4668c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0feec43b52214fb4bed3305363e48d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48db110a2a2b4f879230754ed2dfe60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0f4f804249453e9dae16c637c641d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the future of AI, it would be like having a computer that could talk to humans. Now it would be like having your own AI. But it would be like a computer that could talk to you.\n",
      "\n",
      "The real question is: Do human-AI interactions work?\n",
      "\n",
      "The answer is YES. It's not going to work. It just won't. We're going to have to change the way we think about our own AI.\n",
      "\n",
      "[Interview continues below]\n",
      "\n",
      "This is the question of how to think about the future of artificial intelligence. And I think there is a big problem with that. I think we have to think first about the future of AI.\n",
      "\n",
      "The problem is: How do we think about AI? Are we going to have to think about AI? And how do we think about AI? I think we're going to have to think about AI. I think we're going to have to think about AI. AI is going to be a huge topic in the next few years, but I think there will be enormous amount of work that we need to do to try to get it right.\n",
      "\n",
      "I think there will be a lot of work that we need to do to figure out the best way to think about AI. And I think we're\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "# Generating Text\n",
    "result = generator(\n",
    "    \"In the future of AI,\", # prompt\n",
    "    max_length=50, # max tokens to generate\n",
    "    num_return_sequences=1, # ho many ompletions to return\n",
    "    temperature=0.7 # randomness (lower = more focussed, higher = more creative)\n",
    ")\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625ee6c-d17e-44d3-a2a6-bfe8867764ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
